syntax = "proto3";

package llm.service;

// import "grpc/protos/llm/prompt.proto";

// Ways to configure a prompt request.
message PromptConfig {

}

// A request for llm streaming generation.
message PromptRequest {
  optional string id = 1;
  string content = 2;
  PromptConfig config = 3;
}

// Data about the generation process and the model.
message PromptMetaData {

  message Model {
    string device = 1;
    string dtype = 2;
    string type = 3;
    int32 number_of_devices = 4;
  }

  float tokens_per_second = 1;
  float average_batch_size = 2;
  Model model = 3;

}

// A generated chuck where if is_end_of_sequence is True it will be the last of the stream.
message PromptReply {
  string id = 1;
  string content = 2;
  bool is_end_of_sequence = 3;
  PromptConfig config = 4;
  PromptMetaData meta = 5;
}


service Llm {

  // Ask a llm a question. (The content of the message is given to the model exactly as sent, no formatting or templating)
  rpc prompt(PromptRequest) returns (stream PromptReply);

}